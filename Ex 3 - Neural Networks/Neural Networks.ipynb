{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In the previous part of this exercise, you implemented multi-class logistic regression to recognize handwritten digits. However, logistic regression cannot form more complex hypotheses as it is only a linear classifier.3\n",
    "In this part of the exercise, you will implement a neural network to recognize handwritten digits using the same training set as before. The neural network will be able to represent complex models that form non-linear hypotheses. For this week, you will be using parameters from a neural network that we have already trained. Your goal is to implement the feedforward propagation algorithm to use our weights for prediction.\"\n",
    "\n",
    "\"Neural Network has 3 layers â€“ an input layer, a hidden layer and an output layer. ...You have been provided with a set of network parameters ($\\theta^1$, $\\theta^2$)) already trained by us. These are stored in ex3weights.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "weights = sio.loadmat('../data/ex3weights.mat')\n",
    "theta1, theta2 = weights['Theta1'], weights['Theta2']\n",
    "theta = [theta1, theta2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dimensions of theta1 and theta2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1.shape, theta2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also import the data itself as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sio.loadmat('../data/ex3data1.mat')\n",
    "X = np.float64(data[\"X\"])\n",
    "y = np.float64(data[\"y\"])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have three layers. First one is input with 400 elements. Then we have a middle layer with 25 elements. Finally 10 nods are for the result. This will be a simple case of computation of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def h(theta, X):\n",
    "    \"\"\"\n",
    "    Hypothesis Function where\n",
    "    X is an n x k_prev dimensional array of explanatory variables\n",
    "    theta is k_prev x k_new elements vector\n",
    "    Result will be one dimensional vector of n variables\n",
    "    \"\"\"\n",
    "    return sigmoid(np.dot(X, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the forward propagation mechanism. It will take in theta, which has a dimensionality of $k_{new}$x$k_{old}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, theta):\n",
    "    \"\"\"\n",
    "    A function that will take input matrix X, and move it one layer on \n",
    "    in the neural network.\n",
    "    X will be a n x k_prev matrix - n entries, k_prev properties\n",
    "    theta will be a k_new x k_prev matrix. Function will transpose this \n",
    "    to use in hypothesis functions\n",
    "    \"\"\"\n",
    "    #insert ones to data matrix\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    #transpose theta to feed into hypothesis function\n",
    "    theta = theta.T\n",
    "    return h(theta, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mid and final nodes and store them in a list called Xs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = [X]\n",
    "for i in range(2):\n",
    "    iterated = forward_propagation(Xs[i], theta[i])\n",
    "    Xs.append(iterated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last element of Xs will provide the probabilities. Let's locate the max probability it assigns for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob_argmax = np.argmax(X3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the way the data is arranged we have a mismatch with y and final element of list Xs. 1 in the dataset correspond to 0; 2 to 1; 3 to 2 and so on. Also 0 in images, which correspond to 10 in the dataset here correspond to 9. Good news is that, I can add one to each number and it will solve my problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prob_argmax += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally calculate the accuracy rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9752"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_corrects = np.sum( (y.flatten() == np.float64(Prob_argmax)) )\n",
    "total_dpoints = X.shape[0]\n",
    "\n",
    "accuracy_rate = total_corrects/total_dpoints\n",
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 97.5% accuracy rate. Which is slight improvement over our logistic regression. In Andrew Ng's calculations Logistic regression only gave 94.5% accuracy, which was due to the library of optimization used by him and me are different. On the other hand real test should be done on a separate test set. We probably are overfitting here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
